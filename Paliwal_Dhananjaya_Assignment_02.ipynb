{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DhannajayaPaliwal12/Dhananjaya_INFO5731_Fall2025/blob/main/Paliwal_Dhananjaya_Assignment_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Monday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (25 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "\n",
        "(3) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(4) Collect all the information of the 904 narrators in the Densho Digital Repository.\n",
        "\n",
        "(5)**Collect a total of 10000 reviews** of the top 100 most popular software from G2 and Capterra.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install selenium\n",
        "# ! pip install webdriver-manager\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.firefox.service import Service\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from webdriver_manager.firefox import GeckoDriverManager\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "def scrape_imdb_reviews(url, scroll_pause=2, max_scrolls=20):\n",
        "    ### Setting up configurations for the webdriver\n",
        "    options = webdriver.FirefoxOptions()\n",
        "    options.add_argument(\"--headless\")\n",
        "    driver = webdriver.Firefox(\n",
        "        options=options\n",
        "    )\n",
        "    wait = WebDriverWait(driver, 10)\n",
        "\n",
        "    print(f\"Opening page: {url}\")\n",
        "    driver.get(url)\n",
        "\n",
        "    ### Click the main \"See all\" button\n",
        "    try:\n",
        "        see_all_btn = wait.until(\n",
        "            EC.element_to_be_clickable((By.XPATH, \"//span[contains(@class,'ipc-see-more__text') and text()='See all']\"))\n",
        "        )\n",
        "        driver.execute_script(\"arguments[0].click();\", see_all_btn)\n",
        "        print(\"Clicked the 'See all' button to view all reviews\")\n",
        "        time.sleep(3)\n",
        "    except Exception:\n",
        "        print(\"Could not find 'See all' button, scraping current page only\")\n",
        "\n",
        "    ### Scrolling to load all reviews\n",
        "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "    scrolls = 0\n",
        "    while scrolls < max_scrolls:\n",
        "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "        time.sleep(scroll_pause)\n",
        "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "        if new_height == last_height:\n",
        "            print(\"Reached bottom of page\")\n",
        "            break\n",
        "        last_height = new_height\n",
        "        scrolls += 1\n",
        "        print(f\"Completed {scrolls} scrolls\")\n",
        "\n",
        "    ### Expand the spoiler reviews\n",
        "    print(\"Expanding spoiler reviews\")\n",
        "    spoiler_buttons = driver.find_elements(By.CSS_SELECTOR, \"button.review-spoiler-button\")\n",
        "    expanded = 0\n",
        "    for btn in spoiler_buttons:\n",
        "        try:\n",
        "            driver.execute_script(\"arguments[0].click();\", btn)\n",
        "            expanded += 1\n",
        "            time.sleep(0.3)\n",
        "        except Exception:\n",
        "            pass\n",
        "    print(f\"✅ Expanded {expanded} spoiler reviews\")\n",
        "\n",
        "    ### Collecting all reviews containers\n",
        "    print(\"Extracting review data from all containers\")\n",
        "    reviews = []\n",
        "    review_blocks = driver.find_elements(By.CSS_SELECTOR, \"article.user-review-item\")\n",
        "\n",
        "    for idx, block in enumerate(review_blocks, 1):\n",
        "        if idx <= 1000:\n",
        "          try:\n",
        "              rating = block.find_element(By.CSS_SELECTOR, \".ipc-rating-star--rating\").text\n",
        "          except:\n",
        "              rating = None\n",
        "          try:\n",
        "              title = block.find_element(By.CSS_SELECTOR, \"div[data-testid='review-summary']\").text\n",
        "          except:\n",
        "              title = None\n",
        "          try:\n",
        "              author = block.find_element(By.CSS_SELECTOR, \"a[data-testid='author-link']\").text\n",
        "          except:\n",
        "              author = None\n",
        "          try:\n",
        "              date = block.find_element(By.CSS_SELECTOR, \"li.review-date\").text\n",
        "          except:\n",
        "              date = None\n",
        "          try:\n",
        "              text = block.find_element(By.CSS_SELECTOR, \"div.ipc-html-content\").text\n",
        "          except:\n",
        "              text = None\n",
        "          try:\n",
        "              helpful = block.find_element(By.CSS_SELECTOR, \".ipc-voting__label__count--up\").text\n",
        "          except:\n",
        "              helpful = None\n",
        "\n",
        "          reviews.append({\n",
        "              \"rating\": rating,\n",
        "              \"title\": title,\n",
        "              \"author\": author,\n",
        "              \"date\": date,\n",
        "              \"text\": text,\n",
        "              \"helpful\": helpful\n",
        "          })\n",
        "\n",
        "          if idx % 100 == 0:\n",
        "              print(f\"Extracted {len(reviews)} reviews...\")\n",
        "\n",
        "    driver.quit()\n",
        "    print(f\"Finished Extracting {len(reviews)} reviews.\")\n",
        "    return reviews\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    imdb_url = \"https://www.imdb.com/title/tt15398776/reviews/\"\n",
        "    data = scrape_imdb_reviews(imdb_url, scroll_pause=2, max_scrolls=30)\n",
        "    data = pd.DataFrame(data)\n",
        "\n",
        "    data.to_csv(\"imdb_reviews.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "B2RZWu__UhLl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79ec9fc6-a150-44ba-aa9d-bb6046bd21c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Opening page: https://www.imdb.com/title/tt15398776/reviews/\n",
            "Clicked the 'See all' button to view all reviews\n",
            "Completed 1 scrolls\n",
            "Completed 2 scrolls\n",
            "Completed 3 scrolls\n",
            "Completed 4 scrolls\n",
            "Completed 5 scrolls\n",
            "Completed 6 scrolls\n",
            "Completed 7 scrolls\n",
            "Completed 8 scrolls\n",
            "Completed 9 scrolls\n",
            "Completed 10 scrolls\n",
            "Completed 11 scrolls\n",
            "Completed 12 scrolls\n",
            "Completed 13 scrolls\n",
            "Completed 14 scrolls\n",
            "Completed 15 scrolls\n",
            "Completed 16 scrolls\n",
            "Completed 17 scrolls\n",
            "Completed 18 scrolls\n",
            "Completed 19 scrolls\n",
            "Completed 20 scrolls\n",
            "Completed 21 scrolls\n",
            "Completed 22 scrolls\n",
            "Completed 23 scrolls\n",
            "Completed 24 scrolls\n",
            "Completed 25 scrolls\n",
            "Completed 26 scrolls\n",
            "Completed 27 scrolls\n",
            "Completed 28 scrolls\n",
            "Completed 29 scrolls\n",
            "Completed 30 scrolls\n",
            "Expanding spoiler reviews\n",
            "✅ Expanded 140 spoiler reviews\n",
            "Extracting review data from all containers\n",
            "Extracted 100 reviews...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (15 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5QX6bJjGWXY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5802d9dc-768f-4d98-dc8e-c644f04aaaa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing noise from the reviews text\n",
            "                                                   text  \\\n",
            "0    You'll have to have your wits about you and yo...   \n",
            "1    One of the most anticipated films of the year ...   \n",
            "2    I'm a big fan of Nolan's work so was really lo...   \n",
            "3    I'm still collecting my thoughts after experie...   \n",
            "4    \"Oppenheimer\" is a biographical thriller film ...   \n",
            "..                                                 ...   \n",
            "995  Simply extraordinary, a film that goes beyond ...   \n",
            "996  I was fully in since the first scene, what an ...   \n",
            "997  For films like Oppenheimer, we fell in love wi...   \n",
            "998  \"Oppenheimer,\" directed by Christopher Nolan, ...   \n",
            "999  In summary, Oppenheimer fulfils its promise of...   \n",
            "\n",
            "                                              no_noise  \n",
            "0    You ll have to have your wits about you and yo...  \n",
            "1    One of the most anticipated films of the year ...  \n",
            "2    I m a big fan of Nolan s work so was really lo...  \n",
            "3    I m still collecting my thoughts after experie...  \n",
            "4     Oppenheimer  is a biographical thriller film ...  \n",
            "..                                                 ...  \n",
            "995  Simply extraordinary  a film that goes beyond ...  \n",
            "996  I was fully in since the first scene  what an ...  \n",
            "997  For films like Oppenheimer  we fell in love wi...  \n",
            "998   Oppenheimer   directed by Christopher Nolan  ...  \n",
            "999  In summary  Oppenheimer fulfils its promise of...  \n",
            "\n",
            "[1000 rows x 2 columns]\n",
            "Removing numbers from the reviews text\n",
            "                                                   text  \\\n",
            "0    You'll have to have your wits about you and yo...   \n",
            "1    One of the most anticipated films of the year ...   \n",
            "2    I'm a big fan of Nolan's work so was really lo...   \n",
            "3    I'm still collecting my thoughts after experie...   \n",
            "4    \"Oppenheimer\" is a biographical thriller film ...   \n",
            "..                                                 ...   \n",
            "995  Simply extraordinary, a film that goes beyond ...   \n",
            "996  I was fully in since the first scene, what an ...   \n",
            "997  For films like Oppenheimer, we fell in love wi...   \n",
            "998  \"Oppenheimer,\" directed by Christopher Nolan, ...   \n",
            "999  In summary, Oppenheimer fulfils its promise of...   \n",
            "\n",
            "                                            no_numbers  \n",
            "0    You ll have to have your wits about you and yo...  \n",
            "1    One of the most anticipated films of the year ...  \n",
            "2    I m a big fan of Nolan s work so was really lo...  \n",
            "3    I m still collecting my thoughts after experie...  \n",
            "4     Oppenheimer  is a biographical thriller film ...  \n",
            "..                                                 ...  \n",
            "995  Simply extraordinary  a film that goes beyond ...  \n",
            "996  I was fully in since the first scene  what an ...  \n",
            "997  For films like Oppenheimer  we fell in love wi...  \n",
            "998   Oppenheimer   directed by Christopher Nolan  ...  \n",
            "999  In summary  Oppenheimer fulfils its promise of...  \n",
            "\n",
            "[1000 rows x 2 columns]\n",
            "Removing stopwords from the reviews text\n",
            "                                                   text  \\\n",
            "0    You'll have to have your wits about you and yo...   \n",
            "1    One of the most anticipated films of the year ...   \n",
            "2    I'm a big fan of Nolan's work so was really lo...   \n",
            "3    I'm still collecting my thoughts after experie...   \n",
            "4    \"Oppenheimer\" is a biographical thriller film ...   \n",
            "..                                                 ...   \n",
            "995  Simply extraordinary, a film that goes beyond ...   \n",
            "996  I was fully in since the first scene, what an ...   \n",
            "997  For films like Oppenheimer, we fell in love wi...   \n",
            "998  \"Oppenheimer,\" directed by Christopher Nolan, ...   \n",
            "999  In summary, Oppenheimer fulfils its promise of...   \n",
            "\n",
            "                                          no_stopwords  \n",
            "0    wits brain fully switched watching Oppenheimer...  \n",
            "1    One anticipated films year many people include...  \n",
            "2    big fan Nolan work really looking forward unde...  \n",
            "3    still collecting thoughts experiencing film Ci...  \n",
            "4    Oppenheimer biographical thriller film written...  \n",
            "..                                                 ...  \n",
            "995  Simply extraordinary film goes beyond historic...  \n",
            "996  fully since first scene awesome movie favourit...  \n",
            "997  films like Oppenheimer fell love cinema short ...  \n",
            "998  Oppenheimer directed Christopher Nolan masterf...  \n",
            "999  summary Oppenheimer fulfils promise portraying...  \n",
            "\n",
            "[1000 rows x 2 columns]\n",
            "Lowercasing all texts\n",
            "                                                   text  \\\n",
            "0    You'll have to have your wits about you and yo...   \n",
            "1    One of the most anticipated films of the year ...   \n",
            "2    I'm a big fan of Nolan's work so was really lo...   \n",
            "3    I'm still collecting my thoughts after experie...   \n",
            "4    \"Oppenheimer\" is a biographical thriller film ...   \n",
            "..                                                 ...   \n",
            "995  Simply extraordinary, a film that goes beyond ...   \n",
            "996  I was fully in since the first scene, what an ...   \n",
            "997  For films like Oppenheimer, we fell in love wi...   \n",
            "998  \"Oppenheimer,\" directed by Christopher Nolan, ...   \n",
            "999  In summary, Oppenheimer fulfils its promise of...   \n",
            "\n",
            "                                             lowercase  \n",
            "0    wits brain fully switched watching oppenheimer...  \n",
            "1    one anticipated films year many people include...  \n",
            "2    big fan nolan work really looking forward unde...  \n",
            "3    still collecting thoughts experiencing film ci...  \n",
            "4    oppenheimer biographical thriller film written...  \n",
            "..                                                 ...  \n",
            "995  simply extraordinary film goes beyond historic...  \n",
            "996  fully since first scene awesome movie favourit...  \n",
            "997  films like oppenheimer fell love cinema short ...  \n",
            "998  oppenheimer directed christopher nolan masterf...  \n",
            "999  summary oppenheimer fulfils promise portraying...  \n",
            "\n",
            "[1000 rows x 2 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming\n",
            "                                                   text  \\\n",
            "0    You'll have to have your wits about you and yo...   \n",
            "1    One of the most anticipated films of the year ...   \n",
            "2    I'm a big fan of Nolan's work so was really lo...   \n",
            "3    I'm still collecting my thoughts after experie...   \n",
            "4    \"Oppenheimer\" is a biographical thriller film ...   \n",
            "..                                                 ...   \n",
            "995  Simply extraordinary, a film that goes beyond ...   \n",
            "996  I was fully in since the first scene, what an ...   \n",
            "997  For films like Oppenheimer, we fell in love wi...   \n",
            "998  \"Oppenheimer,\" directed by Christopher Nolan, ...   \n",
            "999  In summary, Oppenheimer fulfils its promise of...   \n",
            "\n",
            "                                               stemmed  \n",
            "0    wit brain fulli switch watch oppenheim could e...  \n",
            "1    one anticip film year mani peopl includ oppenh...  \n",
            "2    big fan nolan work realli look forward underst...  \n",
            "3    still collect thought experienc film cillian m...  \n",
            "4    oppenheim biograph thriller film written direc...  \n",
            "..                                                 ...  \n",
            "995  simpli extraordinari film goe beyond histor pa...  \n",
            "996  fulli sinc first scene awesom movi favourit no...  \n",
            "997  film like oppenheim fell love cinema short opp...  \n",
            "998  oppenheim direct christoph nolan master biogra...  \n",
            "999  summari oppenheim fulfil promis portray stori ...  \n",
            "\n",
            "[1000 rows x 2 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatizing\n",
            "                                                   text  \\\n",
            "0    You'll have to have your wits about you and yo...   \n",
            "1    One of the most anticipated films of the year ...   \n",
            "2    I'm a big fan of Nolan's work so was really lo...   \n",
            "3    I'm still collecting my thoughts after experie...   \n",
            "4    \"Oppenheimer\" is a biographical thriller film ...   \n",
            "..                                                 ...   \n",
            "995  Simply extraordinary, a film that goes beyond ...   \n",
            "996  I was fully in since the first scene, what an ...   \n",
            "997  For films like Oppenheimer, we fell in love wi...   \n",
            "998  \"Oppenheimer,\" directed by Christopher Nolan, ...   \n",
            "999  In summary, Oppenheimer fulfils its promise of...   \n",
            "\n",
            "                                            lemmatized  \n",
            "0    wit brain fulli switch watch oppenheim could e...  \n",
            "1    one anticip film year mani peopl includ oppenh...  \n",
            "2    big fan nolan work realli look forward underst...  \n",
            "3    still collect thought experienc film cillian m...  \n",
            "4    oppenheim biograph thriller film written direc...  \n",
            "..                                                 ...  \n",
            "995  simpli extraordinari film goe beyond histor pa...  \n",
            "996  fulli sinc first scene awesom movi favourit no...  \n",
            "997  film like oppenheim fell love cinema short opp...  \n",
            "998  oppenheim direct christoph nolan master biogra...  \n",
            "999  summari oppenheim fulfil promis portray stori ...  \n",
            "\n",
            "[1000 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from textblob import Word\n",
        "\n",
        "imdb_data = pd.read_csv(\"imdb_reviews.csv\")\n",
        "\n",
        "### Removing noise from the reviews text.\n",
        "def remove_special_characters(text):\n",
        "    return re.sub(r\"[^a-zA-Z0-9\\s]\", \" \", str(text))\n",
        "\n",
        "imdb_data[\"no_noise\"] = imdb_data[\"text\"].apply(remove_special_characters)\n",
        "print(\"Removing noise from the reviews text\\n\", imdb_data[[\"text\", \"no_noise\"]])\n",
        "\n",
        "\n",
        "### Removing numbers from the reviews text.\n",
        "def remove_numbers(text):\n",
        "    return re.sub(r\"\\d+\", \" \", str(text))\n",
        "\n",
        "imdb_data[\"no_numbers\"] = imdb_data[\"no_noise\"].apply(remove_numbers)\n",
        "print(\"Removing numbers from the reviews text\\n\", imdb_data[[\"text\", \"no_numbers\"]])\n",
        "\n",
        "### Removing stopwords by using the stopwords list.\n",
        "nltk.download(\"stopwords\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "def remove_stopwords(text):\n",
        "    tokens = str(text).split()\n",
        "    filtered = [word for word in tokens if word.lower() not in stop_words]\n",
        "    return \" \".join(filtered)\n",
        "\n",
        "imdb_data[\"no_stopwords\"] = imdb_data[\"no_numbers\"].apply(remove_stopwords)\n",
        "print(\"Removing stopwords from the reviews text\\n\", imdb_data[[\"text\", \"no_stopwords\"]])\n",
        "\n",
        "### Lowercase all texts\n",
        "imdb_data[\"lowercase\"] = imdb_data[\"no_stopwords\"].str.lower()\n",
        "print(\"Lowercasing all texts\\n\", imdb_data[[\"text\", \"lowercase\"]])\n",
        "\n",
        "### Stemming\n",
        "st = PorterStemmer()\n",
        "imdb_data[\"stemmed\"] = imdb_data['lowercase'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
        "print(\"Stemming\\n\", imdb_data[[\"text\", \"stemmed\"]])\n",
        "\n",
        "### Lemmatization\n",
        "nltk.download('wordnet')\n",
        "imdb_data['lemmatized'] = imdb_data['stemmed'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
        "print(\"Lemmatizing\\n\", imdb_data[[\"text\", \"lemmatized\"]])\n",
        "\n",
        "imdb_data = imdb_data.drop(columns=[\"no_noise\", \"no_numbers\", \"no_stopwords\", \"lowercase\", \"stemmed\"])\n",
        "imdb_data = imdb_data.rename(columns={\"lemmatized\": \"cleaned_reviews\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (15 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Y0oOSlsOS0cq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fee7552c-cb50-4cf9-db23-3b21992df17c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package benepar_en3 to /root/nltk_data...\n",
            "[nltk_data]   Package benepar_en3 is already up-to-date!\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributions/distribution.py:62: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS counts: {'NOUN': 166, 'VERB': 83, 'ADJ': 61, 'ADV': 30}\n",
            "\n",
            "Dependency Parsing Tree (example sentence)\n",
            "wit          compound   switch       POS=PROPN\n",
            "brain        compound   switch       POS=NOUN\n",
            "fulli        compound   switch       POS=PROPN\n",
            "switch       nsubj      watch        POS=NOUN\n",
            "watch        csubj      absolut      POS=VERB\n",
            "oppenheim    dobj       watch        POS=PROPN\n",
            "could        punct      watch        POS=AUX\n",
            "easili       conj       watch        POS=NOUN\n",
            "get          advcl      watch        POS=VERB\n",
            "away         advmod     get          POS=ADV\n",
            "nonattent    compound   viewer       POS=NOUN\n",
            "viewer       dobj       get          POS=NOUN\n",
            "intellig     advcl      watch        POS=NOUN\n",
            "filmmak      nsubj      show         POS=PROPN\n",
            "show         ccomp      watch        POS=PROPN\n",
            "audienc      prep       watch        POS=PROPN\n",
            "great        amod       respect      POS=ADJ\n",
            "respect      compound   fire         POS=NOUN\n",
            "fire         dobj       watch        POS=NOUN\n",
            "dialogu      amod       inform       POS=NOUN\n",
            "pack         compound   inform       POS=NOUN\n",
            "inform       conj       watch        POS=NOUN\n",
            "relentless   amod       jump         POS=ADJ\n",
            "pace         compound   jump         POS=NOUN\n",
            "jump         dobj       inform       POS=NOUN\n",
            "differ       conj       watch        POS=VERB\n",
            "time         dobj       differ       POS=NOUN\n",
            "oppenheim    compound   life         POS=PROPN\n",
            "life         nsubj      continu      POS=NOUN\n",
            "continu      relcl      time         POS=VERB\n",
            "hour         compound   runtim       POS=NOUN\n",
            "runtim       nmod       clue         POS=NOUN\n",
            "visual       amod       clue         POS=ADJ\n",
            "clue         compound   guid         POS=PROPN\n",
            "guid         dobj       continu      POS=PROPN\n",
            "viewer       compound   time         POS=PROPN\n",
            "time         npadvmod   continu      POS=NOUN\n",
            "get          dep        watch        POS=VERB\n",
            "grip         nsubj      quit         POS=NOUN\n",
            "quit         ccomp      get          POS=VERB\n",
            "quickli      compound   help         POS=PROPN\n",
            "relentless   compound   help         POS=NOUN\n",
            "help         dobj       quit         POS=NOUN\n",
            "express      advcl      watch        POS=VERB\n",
            "urgenc       amod       germani      POS=ADJ\n",
            "u            nmod       germani      POS=PROPN\n",
            "attack       compound   chase        POS=NOUN\n",
            "chase        compound   germani      POS=NOUN\n",
            "atom         compound   bomb         POS=PROPN\n",
            "bomb         compound   germani      POS=NOUN\n",
            "germani      dobj       express      POS=PROPN\n",
            "could        aux        absolut      POS=AUX\n",
            "absolut      ccomp      say          POS=VERB\n",
            "career       dobj       absolut      POS=NOUN\n",
            "best         advmod     perform      POS=ADJ\n",
            "perform      compound   oscar        POS=NOUN\n",
            "consistenli  compound   brilliant    POS=PROPN\n",
            "brilliant    compound   oscar        POS=PROPN\n",
            "cillian      compound   oscar        POS=PROPN\n",
            "murphi       compound   oscar        POS=PROPN\n",
            "anchor       compound   oscar        POS=PROPN\n",
            "film         compound   oscar        POS=NOUN\n",
            "nail         compound   oscar        POS=NOUN\n",
            "oscar        nsubj      perform      POS=NOUN\n",
            "perform      conj       absolut      POS=VERB\n",
            "fact         dobj       perform      POS=NOUN\n",
            "whole        amod       cast         POS=ADJ\n",
            "cast         npadvmod   perform      POS=NOUN\n",
            "fantast      acl        cast         POS=VERB\n",
            "apart        advmod     fantast      POS=ADV\n",
            "mayb         compound   blunt        POS=PROPN\n",
            "sometim      amod       blunt        POS=PROPN\n",
            "overwrought  amod       blunt        POS=PROPN\n",
            "emili        compound   blunt        POS=PROPN\n",
            "blunt        nsubj      perform      POS=NOUN\n",
            "perform      conj       absolut      POS=VERB\n",
            "rdj          dobj       perform      POS=ADJ\n",
            "also         advmod     brilliant    POS=ADV\n",
            "particularli amod       brilliant    POS=ADJ\n",
            "brilliant    amod       return       POS=ADJ\n",
            "return       compound   layer        POS=NOUN\n",
            "proper       amod       call         POS=ADJ\n",
            "act          compound   call         POS=NOUN\n",
            "decad        compound   call         POS=PROPN\n",
            "call         compound   layer        POS=VERB\n",
            "screenplay   compound   layer        POS=NOUN\n",
            "den          compound   layer        POS=NOUN\n",
            "layer        nsubj      say          POS=NOUN\n",
            "say          ROOT       say          POS=VERB\n",
            "thick        amod       cinematographi POS=ADJ\n",
            "bibl         compound   cinematographi POS=ADJ\n",
            "cinematographi nsubj      quit         POS=NOUN\n",
            "quit         ccomp      say          POS=VERB\n",
            "stark        amod       part         POS=ADJ\n",
            "spare        amod       part         POS=ADJ\n",
            "part         compound   imbu         POS=NOUN\n",
            "imbu         nmod       moment       POS=PROPN\n",
            "rich         amod       luciou       POS=ADJ\n",
            "luciou       appos      imbu         POS=PROPN\n",
            "colour       compound   moment       POS=NOUN\n",
            "moment       nsubj      especi       POS=NOUN\n",
            "especi       ccomp      quit         POS=VERB\n",
            "scene        dobj       especi       POS=NOUN\n",
            "florenc      npadvmod   especi       POS=PROPN\n",
            "pugh         amod       beauti       POS=ADJ\n",
            "score        compound   beauti       POS=NOUN\n",
            "beauti       compound   time         POS=PROPN\n",
            "time         npadvmod   especi       POS=PROPN\n",
            "mostli       compound   oppress      POS=PROPN\n",
            "anxiou       compound   oppress      POS=PROPN\n",
            "oppress      compound   hour         POS=NOUN\n",
            "ad           nmod       hour         POS=NOUN\n",
            "relentless   amod       pace         POS=ADJ\n",
            "pace         compound   hour         POS=NOUN\n",
            "hour         nmod       fli          POS=NOUN\n",
            "runtim       compound   fli          POS=NOUN\n",
            "fli          nsubj      found        POS=NOUN\n",
            "found        conj       quit         POS=VERB\n",
            "intens       compound   reward       POS=NOUN\n",
            "tax          compound   highli       POS=NOUN\n",
            "highli       compound   reward       POS=PROPN\n",
            "reward       nsubj      watch        POS=NOUN\n",
            "watch        ccomp      found        POS=VERB\n",
            "film         dobj       watch        POS=NOUN\n",
            "make         ccomp      say          POS=VERB\n",
            "finest       amod       watch        POS=ADJ\n",
            "realli       nmod       watch        POS=PROPN\n",
            "great        amod       watch        POS=PROPN\n",
            "watch        dobj       make         POS=NOUN\n",
            "\n",
            "Constituency Parsing Tree (example sentence)\n",
            "(S (FW wit) (NN brain) (JJ fulli) (NN switch) (NP (NN watch) (NP (NNP oppenheim))) (VP (MD could) (NP (RB easili)) (VB get) (ADVP (RB away)) (ADJP (NP (JJ nonattent) (NN viewer)) (JJ intellig)) (NN filmmak) (NN show) (FW audienc) (JJ great) (NN respect) (NN fire) (NN dialogu) (NN pack) (VB inform) (JJ relentless) (NN pace) (NN jump) (JJ differ) (NN time) (NNP oppenheim) (NN life) (NP (NN continu) (NN hour)) (NN runtim) (JJ visual) (NN clue) (JJ guid) (NP (NN viewer)) (NN time) (VP (VB get) (VB grip)) (RB quit) (, quickli) (JJ relentless) (NN help) (VB express) (NN urgenc) (RB u) (NN attack) (NN chase) (NN atom) (NN bomb) (RB germani) (VP (MD could) (RB absolut) (NN career) (JJS best) (VB perform) (IN consistenli) (JJ brilliant) (NN cillian) (NN murphi) (NN anchor) (NN film) (NN nail) (NN oscar) (VB perform) (NN fact) (JJ whole) (NN cast) (. fantast) (RP apart) (VB mayb) (ADVP (RB sometim) (ADVP (VBN overwrought) (JJ emili) (JJ blunt) (VB perform) (ADVP (NP (NN rdj)) (ADVP (RB also))))) (VP (IN particularli) (JJ brilliant) (VB return) (JJ proper) (NN act) (VBN decad) (VB call) (NN screenplay) (PP (DT den) (NN layer) (PP (VB say) (NP (NP (JJ thick) (NN bibl) (NNS cinematographi)) (VP (VBP quit) (NP (NP (JJ stark) (JJ spare) (NN part) (VB imbu) (NP (NML (NP (NP (NP (NP (ADJP (JJ rich) (JJ luciou)) (NN colour)) (PP (NN moment) (FW especi) (NN scene) (FW florenc))) (NN pugh) (NN score)) (RB beauti)) (NN time) (RB mostli) (FW anxiou) (NN oppress) (RB ad) (JJ relentless) (NN pace)) (NN hour) (RB runtim) (IN fli) (VBN found) (NP (NP (JJ intens) (. tax) (. highli) (NN reward) (NN watch)) (NP (NN film))))) (S (VP (VB make) (NP (JJS finest) (. realli) (JJ great) (NN watch)))))))))))))\n",
            "\n",
            "Explanation:\n",
            "- Dependency Parsing shows grammatical links, with 'say' as the ROOT and other words\n",
            "like 'quit' or 'make' connected as clauses. It also captures relations such as 'viewer'\n",
            "being the object of 'get' and 'film' the object of 'watch'.\n",
            "- Constituency Parsing groups words into phrases, e.g., (NP viewer) as a noun phrase\n",
            "and (VP could get away) as a verb phrase. This reveals how nouns, verbs, and modifiers\n",
            "combine to build the sentence structure.\n",
            "\n",
            "NER counts: {'GPE': 4, 'TIME': 5, 'PERSON': 10, 'NORP': 3, 'CARDINAL': 5, 'ORG': 3, 'ORDINAL': 2, 'DATE': 2, 'EVENT': 1}\n"
          ]
        }
      ],
      "source": [
        "# !pip install benepar\n",
        "\n",
        "import spacy\n",
        "from collections import Counter\n",
        "import benepar\n",
        "\n",
        "benepar.download('benepar_en3')\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# add benepar (this sets up constituency parsing)\n",
        "nlp.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})\n",
        "\n",
        "texts = imdb_data[\"cleaned_reviews\"].dropna().tolist()[:4]\n",
        "\n",
        "docs = list(nlp.pipe(texts))  # these are Doc objects\n",
        "\n",
        "### POS tagging\n",
        "pos_counts = Counter([token.pos_ for doc in docs for token in doc])\n",
        "print(\"POS counts:\", {k: pos_counts[k] for k in [\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"]})\n",
        "\n",
        "### Dependency Parsing (first sentence of first doc)\n",
        "print(\"\\nDependency Parsing Tree (example sentence)\")\n",
        "first_doc = docs[0]\n",
        "first_sent = list(first_doc.sents)[0]  # take first sentence span\n",
        "for token in first_sent:\n",
        "    print(f\"{token.text:<12} {token.dep_:<10} {token.head.text:<12} POS={token.pos_}\")\n",
        "\n",
        "### Constituency Parsing (first sentence only)\n",
        "print(\"\\nConstituency Parsing Tree (example sentence)\")\n",
        "print(first_sent._.parse_string)   # works now, since it's a Span\n",
        "\n",
        "print(\"\"\"\n",
        "Explanation:\n",
        "- Dependency Parsing shows grammatical links, with 'say' as the ROOT and other words\n",
        "like 'quit' or 'make' connected as clauses. It also captures relations such as 'viewer'\n",
        "being the object of 'get' and 'film' the object of 'watch'.\n",
        "- Constituency Parsing groups words into phrases, e.g., (NP viewer) as a noun phrase\n",
        "and (VP could get away) as a verb phrase. This reveals how nouns, verbs, and modifiers\n",
        "combine to build the sentence structure.\n",
        "\"\"\")\n",
        "\n",
        "### Named Entity Recognition\n",
        "ner_counts = Counter([ent.label_ for doc in docs for ent in doc.ents])\n",
        "print(\"NER counts:\", dict(ner_counts))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Following Questions must answer using AI assitance**"
      ],
      "metadata": {
        "id": "EcVqy1yj3wja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 4 (20 points)."
      ],
      "metadata": {
        "id": "kEdcyHX8VaDB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. (PART-1)\n",
        "Web scraping data from the GitHub Marketplace to gather details about popular actions. Using Python, the process begins by sending HTTP requests to multiple pages of the marketplace (1000 products), handling pagination through dynamic page numbers. The key details extracted include the product name, a short description, and the URL.\n",
        "\n",
        " The extracted data is stored in a structured CSV format with columns for product name, description, URL, and page number. A time delay is introduced between requests to avoid server overload. ChatGPT can assist by helping with the parsing of HTML, error handling, and generating reports based on the data collected.\n",
        "\n",
        " The goal is to complete the scraping within a specified time limit, ensuring that the process is efficient and adheres to GitHub’s usage guidelines.\n",
        "\n",
        "(PART -2)\n",
        "\n",
        "1.   **Preprocess Data**: Clean the text by tokenizing, removing stopwords, and converting to lowercase.\n",
        "\n",
        "2. Perform **Data Quality** operations.\n",
        "\n",
        "\n",
        "Preprocessing:\n",
        "Preprocessing involves cleaning the text by removing noise such as special characters, HTML tags, and unnecessary whitespace. It also includes tasks like tokenization, stopword removal, and lemmatization to standardize the text for analysis.\n",
        "\n",
        "Data Quality:\n",
        "Data quality checks ensure completeness, consistency, and accuracy by verifying that all required columns are filled and formatted correctly. Additionally, it involves identifying and removing duplicates, handling missing values, and ensuring the data reflects the true content accurately.\n"
      ],
      "metadata": {
        "id": "1Ung5_YW3C6y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Github MarketPlace page:\n",
        "https://github.com/marketplace?type=actions"
      ],
      "metadata": {
        "id": "CTOfUpatronW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_github_marketplace(pages=20, delay=2):\n",
        "    ### Setting up configurations for the webdriver\n",
        "    options = webdriver.FirefoxOptions()\n",
        "    options.set_preference(\"general.useragent.override\",\n",
        "                       \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "                       \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "                       \"Chrome/122.0.0.0 Safari/537.36\")\n",
        "    options.add_argument(\"--headless\")\n",
        "\n",
        "    driver = webdriver.Firefox(\n",
        "        options=options\n",
        "    )\n",
        "    wait = WebDriverWait(driver, 10)\n",
        "    base_url = \"https://github.com/marketplace?type=actions&page={}\"\n",
        "\n",
        "    data = []\n",
        "    ### Starting to go through each page\n",
        "    for page in range(1, pages + 1):\n",
        "        url = base_url.format(page)\n",
        "        print(f\"Opening page {page}: {url}\")\n",
        "        driver.get(url)\n",
        "        time.sleep(delay)\n",
        "\n",
        "        try:\n",
        "            ### Reading the html content of the page\n",
        "            html = driver.page_source\n",
        "            wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[href*='/actions/']\")))\n",
        "        except Exception as e:\n",
        "            print(f\"Timeout loading page {page}: {e}\")\n",
        "            continue\n",
        "\n",
        "        cards = driver.find_elements(By.CSS_SELECTOR, 'div[data-testid=\"non-featured-item\"]')\n",
        "        print(\"Found:\", len(cards))\n",
        "\n",
        "        for card in cards:\n",
        "          try:\n",
        "              ### Getting the required product link and product_name along with description\n",
        "              link_elem = card.find_element(By.CSS_SELECTOR, \"a.marketplace-common-module__marketplace-item-link--AeQSq\")\n",
        "              product_name = link_elem.text.strip()\n",
        "              product_url = link_elem.get_attribute(\"href\")\n",
        "          except:\n",
        "              product_name, product_url = None, None\n",
        "\n",
        "          try:\n",
        "              description = card.find_element(By.CSS_SELECTOR, \"p.fgColor-muted\").text.strip()\n",
        "          except:\n",
        "              description = None\n",
        "\n",
        "          data.append({\n",
        "              \"name\": product_name,\n",
        "              \"url\": product_url,\n",
        "              \"description\": description,\n",
        "              \"page\": page\n",
        "          })\n",
        "\n",
        "        print(f\"Extracted {len(cards)} items from page {page}\")\n",
        "\n",
        "    driver.quit()\n",
        "    print(f\"\\n Finished scraping {len(data)} items.\")\n",
        "    return data\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    scraped_data = scrape_github_marketplace(pages=50, delay=3)  # ~1000 items\n",
        "    df = pd.DataFrame(scraped_data)\n",
        "    df.to_csv(\"github_marketplace_actions.csv\", index=False)\n",
        "    print(\"Data saved to github_marketplace_actions.csv\")\n"
      ],
      "metadata": {
        "id": "4dtco9K--ks6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7e90f0a-2503-4f33-ab15-6e8c10a79d1d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Opening page 1: https://github.com/marketplace?type=actions&page=1\n",
            "Found: 20\n",
            "Extracted 20 items from page 1\n",
            "Opening page 2: https://github.com/marketplace?type=actions&page=2\n",
            "Found: 20\n",
            "Extracted 20 items from page 2\n",
            "Opening page 3: https://github.com/marketplace?type=actions&page=3\n",
            "Found: 20\n",
            "Extracted 20 items from page 3\n",
            "Opening page 4: https://github.com/marketplace?type=actions&page=4\n",
            "Found: 20\n",
            "Extracted 20 items from page 4\n",
            "Opening page 5: https://github.com/marketplace?type=actions&page=5\n",
            "Found: 20\n",
            "Extracted 20 items from page 5\n",
            "Opening page 6: https://github.com/marketplace?type=actions&page=6\n",
            "Found: 20\n",
            "Extracted 20 items from page 6\n",
            "Opening page 7: https://github.com/marketplace?type=actions&page=7\n",
            "Found: 20\n",
            "Extracted 20 items from page 7\n",
            "Opening page 8: https://github.com/marketplace?type=actions&page=8\n",
            "Found: 20\n",
            "Extracted 20 items from page 8\n",
            "Opening page 9: https://github.com/marketplace?type=actions&page=9\n",
            "Found: 20\n",
            "Extracted 20 items from page 9\n",
            "Opening page 10: https://github.com/marketplace?type=actions&page=10\n",
            "Found: 20\n",
            "Extracted 20 items from page 10\n",
            "Opening page 11: https://github.com/marketplace?type=actions&page=11\n",
            "Found: 20\n",
            "Extracted 20 items from page 11\n",
            "Opening page 12: https://github.com/marketplace?type=actions&page=12\n",
            "Found: 20\n",
            "Extracted 20 items from page 12\n",
            "Opening page 13: https://github.com/marketplace?type=actions&page=13\n",
            "Found: 20\n",
            "Extracted 20 items from page 13\n",
            "Opening page 14: https://github.com/marketplace?type=actions&page=14\n",
            "Found: 20\n",
            "Extracted 20 items from page 14\n",
            "Opening page 15: https://github.com/marketplace?type=actions&page=15\n",
            "Found: 20\n",
            "Extracted 20 items from page 15\n",
            "Opening page 16: https://github.com/marketplace?type=actions&page=16\n",
            "Found: 20\n",
            "Extracted 20 items from page 16\n",
            "Opening page 17: https://github.com/marketplace?type=actions&page=17\n",
            "Found: 20\n",
            "Extracted 20 items from page 17\n",
            "Opening page 18: https://github.com/marketplace?type=actions&page=18\n",
            "Found: 20\n",
            "Extracted 20 items from page 18\n",
            "Opening page 19: https://github.com/marketplace?type=actions&page=19\n",
            "Found: 20\n",
            "Extracted 20 items from page 19\n",
            "Opening page 20: https://github.com/marketplace?type=actions&page=20\n",
            "Found: 20\n",
            "Extracted 20 items from page 20\n",
            "Opening page 21: https://github.com/marketplace?type=actions&page=21\n",
            "Found: 20\n",
            "Extracted 20 items from page 21\n",
            "Opening page 22: https://github.com/marketplace?type=actions&page=22\n",
            "Found: 20\n",
            "Extracted 20 items from page 22\n",
            "Opening page 23: https://github.com/marketplace?type=actions&page=23\n",
            "Found: 20\n",
            "Extracted 20 items from page 23\n",
            "Opening page 24: https://github.com/marketplace?type=actions&page=24\n",
            "Found: 20\n",
            "Extracted 20 items from page 24\n",
            "Opening page 25: https://github.com/marketplace?type=actions&page=25\n",
            "Found: 20\n",
            "Extracted 20 items from page 25\n",
            "Opening page 26: https://github.com/marketplace?type=actions&page=26\n",
            "Found: 20\n",
            "Extracted 20 items from page 26\n",
            "Opening page 27: https://github.com/marketplace?type=actions&page=27\n",
            "Found: 20\n",
            "Extracted 20 items from page 27\n",
            "Opening page 28: https://github.com/marketplace?type=actions&page=28\n",
            "Found: 20\n",
            "Extracted 20 items from page 28\n",
            "Opening page 29: https://github.com/marketplace?type=actions&page=29\n",
            "Found: 20\n",
            "Extracted 20 items from page 29\n",
            "Opening page 30: https://github.com/marketplace?type=actions&page=30\n",
            "Found: 20\n",
            "Extracted 20 items from page 30\n",
            "Opening page 31: https://github.com/marketplace?type=actions&page=31\n",
            "Found: 20\n",
            "Extracted 20 items from page 31\n",
            "Opening page 32: https://github.com/marketplace?type=actions&page=32\n",
            "Found: 20\n",
            "Extracted 20 items from page 32\n",
            "Opening page 33: https://github.com/marketplace?type=actions&page=33\n",
            "Found: 20\n",
            "Extracted 20 items from page 33\n",
            "Opening page 34: https://github.com/marketplace?type=actions&page=34\n",
            "Found: 20\n",
            "Extracted 20 items from page 34\n",
            "Opening page 35: https://github.com/marketplace?type=actions&page=35\n",
            "Found: 20\n",
            "Extracted 20 items from page 35\n",
            "Opening page 36: https://github.com/marketplace?type=actions&page=36\n",
            "Found: 20\n",
            "Extracted 20 items from page 36\n",
            "Opening page 37: https://github.com/marketplace?type=actions&page=37\n",
            "Found: 20\n",
            "Extracted 20 items from page 37\n",
            "Opening page 38: https://github.com/marketplace?type=actions&page=38\n",
            "Found: 20\n",
            "Extracted 20 items from page 38\n",
            "Opening page 39: https://github.com/marketplace?type=actions&page=39\n",
            "Found: 20\n",
            "Extracted 20 items from page 39\n",
            "Opening page 40: https://github.com/marketplace?type=actions&page=40\n",
            "Found: 20\n",
            "Extracted 20 items from page 40\n",
            "Opening page 41: https://github.com/marketplace?type=actions&page=41\n",
            "Found: 20\n",
            "Extracted 20 items from page 41\n",
            "Opening page 42: https://github.com/marketplace?type=actions&page=42\n",
            "Found: 20\n",
            "Extracted 20 items from page 42\n",
            "Opening page 43: https://github.com/marketplace?type=actions&page=43\n",
            "Found: 20\n",
            "Extracted 20 items from page 43\n",
            "Opening page 44: https://github.com/marketplace?type=actions&page=44\n",
            "Found: 20\n",
            "Extracted 20 items from page 44\n",
            "Opening page 45: https://github.com/marketplace?type=actions&page=45\n",
            "Found: 20\n",
            "Extracted 20 items from page 45\n",
            "Opening page 46: https://github.com/marketplace?type=actions&page=46\n",
            "Found: 20\n",
            "Extracted 20 items from page 46\n",
            "Opening page 47: https://github.com/marketplace?type=actions&page=47\n",
            "Found: 20\n",
            "Extracted 20 items from page 47\n",
            "Opening page 48: https://github.com/marketplace?type=actions&page=48\n",
            "Found: 20\n",
            "Extracted 20 items from page 48\n",
            "Opening page 49: https://github.com/marketplace?type=actions&page=49\n",
            "Found: 20\n",
            "Extracted 20 items from page 49\n",
            "Opening page 50: https://github.com/marketplace?type=actions&page=50\n",
            "Found: 20\n",
            "Extracted 20 items from page 50\n",
            "\n",
            " Finished scraping 1000 items.\n",
            "Data saved to github_marketplace_actions.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "print(\"Initial shape of the dataframe:\", df.shape)\n",
        "\n",
        "### Performing Data Quality Checks\n",
        "print(\"\\nMissing values per column:\")\n",
        "print(df.isna().sum())\n",
        "\n",
        "print(\"\\nDuplicate rows:\", df.duplicated().sum())\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "### Dropping rows missing critical fields (name or url)\n",
        "df.dropna(subset=[\"name\", \"url\"], inplace=True)\n",
        "\n",
        "print(\"Shape of dataframe after cleaning:\", df.shape)\n",
        "\n",
        "### Preprocessing text\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    ### Removing HTML tags\n",
        "    text = re.sub(r\"<.*?>\", \" \", text)\n",
        "    ### Keepping only letters\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \" \", text)\n",
        "    ### Lowercase\n",
        "    text = text.lower()\n",
        "    ### Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "    ### Remove stopwords + lemmatize\n",
        "    tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "### Apply preprocessing\n",
        "df[\"description_clean\"] = df[\"description\"].astype(str).apply(clean_text)\n",
        "\n",
        "### Getting a quick summary of cleaned text\n",
        "all_words = \" \".join(df[\"description_clean\"]).split()\n",
        "freq = Counter(all_words).most_common(20)\n",
        "\n",
        "print(\"\\nTop 20 most common words in descriptions:\")\n",
        "print(freq)\n",
        "\n",
        "df.to_csv(\"github_marketplace_actions_clean.csv\", index=False)\n",
        "print(\"Saved cleaned data to github_marketplace_actions_clean.csv\")\n"
      ],
      "metadata": {
        "id": "oGamP6Xy5Q7G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6625f841-d1ec-4588-e8fc-2cb0f343c0fb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial shape of the dataframe: (1000, 4)\n",
            "\n",
            "Missing values per column:\n",
            "name           0\n",
            "url            0\n",
            "description    0\n",
            "page           0\n",
            "dtype: int64\n",
            "\n",
            "Duplicate rows: 0\n",
            "Shape of dataframe after cleaning: (1000, 4)\n",
            "\n",
            "Top 20 most common words in descriptions:\n",
            "[('github', 354), ('action', 321), ('run', 118), ('request', 91), ('pull', 85), ('code', 82), ('file', 82), ('build', 67), ('workflow', 63), ('using', 61), ('release', 51), ('deploy', 50), ('repository', 48), ('pr', 45), ('automatically', 45), ('issue', 42), ('project', 41), ('install', 41), ('version', 40), ('check', 40)]\n",
            "Saved cleaned data to github_marketplace_actions_clean.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 5 (20 points)\n",
        "\n",
        "PART 1:\n",
        "Web Scrape  tweets from Twitter using the Tweepy API, specifically targeting hashtags related to subtopics (machine learning or artificial intelligence.)\n",
        "The extracted data includes the tweet ID, username, and text.\n",
        "\n",
        "Part 2:\n",
        "Perform data cleaning procedures\n",
        "\n",
        "A final data quality check ensures the completeness and consistency of the dataset. The cleaned data is then saved into a CSV file for further analysis.\n",
        "\n",
        "\n",
        "**Note**\n",
        "\n",
        "1.   Follow tutorials provided in canvas to obtain api keys. Use ChatGPT to get the code. Make sure the file is downloaded and saved.\n",
        "2.   Make sure you divide GPT code as shown in tutorials, dont make multiple requestes.\n"
      ],
      "metadata": {
        "id": "3WeD70ty3Gui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "from google.colab import userdata\n",
        "\n",
        "BEARER_TOKEN = userdata.get('X_BEARER_TOKEN')\n",
        "\n",
        "client = tweepy.Client(bearer_token=BEARER_TOKEN)\n",
        "\n",
        "def fetch_tweets_for_hashtag(hashtag, max_tweets=100):\n",
        "    query = f\"{hashtag} -is:retweet\"\n",
        "\n",
        "    resp = client.search_recent_tweets(\n",
        "        query=query,\n",
        "        tweet_fields=[\"id\", \"text\", \"author_id\", \"created_at\"],\n",
        "        expansions=[\"author_id\"],\n",
        "        user_fields=[\"username\"],\n",
        "        max_results=50  # max per request\n",
        "    )\n",
        "\n",
        "    results = []\n",
        "    if not resp.data:\n",
        "        return results\n",
        "\n",
        "    ### Build a dict from user_id → user object to map usernames\n",
        "    users = {}\n",
        "    if resp.includes and \"users\" in resp.includes:\n",
        "        for user in resp.includes[\"users\"]:\n",
        "            users[user.id] = user\n",
        "\n",
        "    ### For each tweet, combine with user info\n",
        "    for tw in resp.data:\n",
        "        user = users.get(tw.author_id)\n",
        "        username = user.username if user else None\n",
        "        results.append({\n",
        "            \"tweet_id\": tw.id,\n",
        "            \"username\": username,\n",
        "            \"text\": tw.text\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # hashtags = [\"#machinelearning\", \"#artificialintelligence\"]\n",
        "    hashtags = [\"#machinelearning\"]\n",
        "    all_tweets = []\n",
        "    for i, tag in enumerate(hashtags):\n",
        "        if i > 0:  # Add a delay before fetching the next hashtag\n",
        "            print(f\"\\nPausing for 10 seconds before fetching tweets for {tag}\\n\")\n",
        "            time.sleep(60) # Pause for 10 seconds to avoid rate limits\n",
        "        tweets = fetch_tweets_for_hashtag(tag, max_tweets=100)\n",
        "        print(f\"\\nFetched {len(tweets)} tweets for {tag}\\n\")\n",
        "        all_tweets.extend(tweets)\n",
        "    all_tweets = pd.DataFrame(all_tweets)\n",
        "    all_tweets.to_csv(\"tweets.csv\", index=False)"
      ],
      "metadata": {
        "id": "yp_ISEr5_EMp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "27f09e85-dd5c-43da-813e-afe3e706ed68"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TooManyRequests",
          "evalue": "429 Too Many Requests\nUsage cap exceeded: Monthly product cap",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTooManyRequests\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1702972831.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nPausing for 10 seconds before fetching tweets for {tag}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Pause for 10 seconds to avoid rate limits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mtweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_tweets_for_hashtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_tweets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nFetched {len(tweets)} tweets for {tag}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mall_tweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1702972831.py\u001b[0m in \u001b[0;36mfetch_tweets_for_hashtag\u001b[0;34m(hashtag, max_tweets)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{hashtag} -is:retweet\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     resp = client.search_recent_tweets(\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mtweet_fields\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"text\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"author_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"created_at\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tweepy/client.py\u001b[0m in \u001b[0;36msearch_recent_tweets\u001b[0;34m(self, query, user_auth, **params)\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \"\"\"\n\u001b[1;32m   1275\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"query\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m         return self._make_request(\n\u001b[0m\u001b[1;32m   1277\u001b[0m             \u001b[0;34m\"GET\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/2/tweets/search/recent\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m             endpoint_parameters=(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tweepy/client.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, method, route, params, endpoint_parameters, json, data_type, user_auth)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mrequest_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         response = self.request(method, route, params=request_params,\n\u001b[0m\u001b[1;32m    130\u001b[0m                                 json=json, user_auth=user_auth)\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tweepy/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, route, params, json, user_auth)\u001b[0m\n\u001b[1;32m    113\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroute\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_auth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mTooManyRequests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTwitterServerError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTooManyRequests\u001b[0m: 429 Too Many Requests\nUsage cap exceeded: Monthly product cap"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = pd.read_csv(\"tweets.csv\")\n",
        "print(\"Dataset Description\\n\")\n",
        "print(tweets.info())\n",
        "\n",
        "print(\"\\nFirst 5 rows\\n\", tweets.head())\n",
        "\n",
        "tweets = tweets.drop_duplicates(subset=\"tweet_id\", keep=\"first\")\n",
        "print(\"\\nNumber of Null values in each column\\n\", tweets.isnull().sum())\n",
        "\n",
        "### Dropping rows where text is missing\n",
        "tweets = tweets.dropna(subset=[\"tweet_id\", \"text\"])\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)         # remove URLs\n",
        "    text = re.sub(r\"@\\w+\", \"\", text)            # remove mentions\n",
        "    text = re.sub(r\"#\", \"\", text)               # remove hashtag symbol\n",
        "    text = re.sub(r\"[^A-Za-z0-9\\s]\", \" \", text) # remove emojis/punctuation\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()    # remove extra spaces\n",
        "    return text.lower()\n",
        "\n",
        "tweets[\"clean_text\"] = tweets[\"text\"].astype(str).apply(clean_text)\n",
        "\n",
        "tweets.to_csv(\"tweets_cleaned.csv\", index=False)\n",
        "print(\"\\nCleaned data saved as tweets_cleaned.csv\")"
      ],
      "metadata": {
        "id": "HGX-YGqmeO69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaaffc9d-8fcc-48ed-da94-a91f8703dad2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Description\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 3 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   tweet_id  100 non-null    int64 \n",
            " 1   username  100 non-null    object\n",
            " 2   text      100 non-null    object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 2.5+ KB\n",
            "None\n",
            "\n",
            "First 5 rows\n",
            "               tweet_id         username  \\\n",
            "0  1972832514160153027     rasangarocks   \n",
            "1  1972831317160288326        cyberkeyx   \n",
            "2  1972830137579352403  EffieRitts70463   \n",
            "3  1972830025889292446     Radiology_AI   \n",
            "4  1972830009120453009        Sargol_MD   \n",
            "\n",
            "                                                text  \n",
            "0  Python Become a Master: 120 ‘Real World’ Pytho...  \n",
            "1  If your account is hacked, or your Account is ...  \n",
            "2  @PythonPr That's a great way to approach it! V...  \n",
            "3  Deep learning method for lumbar paraspinal mus...  \n",
            "4  We developed a Machine Learning framework to p...  \n",
            "\n",
            "Number of Null values in each column\n",
            " tweet_id    0\n",
            "username    0\n",
            "text        0\n",
            "dtype: int64\n",
            "\n",
            "Cleaned data saved as tweets_cleaned.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ],
      "metadata": {
        "id": "q8BFCvWp32cf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The webscrapping part in the assignment felt difficult and time consuming as I struggled through it. Most of the websites block such web scrapping attempts or have a limit for the number of records which can be scrapped. It was a nice learning exercise as I had previously not tried my hands over web scrapping data so got to learn a lot through this assignment. Time was sufficient enough to complete this whole thing."
      ],
      "metadata": {
        "id": "CBW_KnSYhgwy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write your response below\n",
        "Fill out survey and provide your valuable feedback.\n",
        "\n",
        "https://docs.google.com/forms/d/e/1FAIpQLSd_ObuA3iNoL7Az_C-2NOfHodfKCfDzHZtGRfIker6WyZqTtA/viewform?usp=dialog"
      ],
      "metadata": {
        "id": "JbTa-jDS-KFI"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}